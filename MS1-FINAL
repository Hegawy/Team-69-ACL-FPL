{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to Team 69-FPL Prediction Notebook\n\nWelcome to our Fantasy Premier League (FPL) prediction project!  \nIn this notebook, **Team 69** explores player and match statistics across multiple seasons to predict upcoming gameweek points using both **Linear Regression** and a **Feed-Forward Neural Network (FFNN)**.\n\nWe’ll walk through:\n-  Data cleaning and preprocessing  \n-  Feature engineering and scaling  \n-  Model training and evaluation  \n-  Explainability with SHAP and LIME  \n-  Final inference using our trained models  \n\nLet’s get started :)\n","metadata":{}},{"cell_type":"markdown","source":"| Name | Student ID |\n|------|-------------|\n| Kerollos Ashraf Nagy | 55-25131 |\n| Yusuf Mohamed Alhegawy | 55-26352 |\n| Youssef Omar Ahmed Aboelela | 55-3390 |\n| Mohamed Alaaeldin Ibrahim Elsherif | 55-1358 |\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import Sequential, Input\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.metrics import Precision, Recall\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fantasy-football/cleaned_merged_seasons.csv')\n\n# Normalize positions (GKP -> GK)\nposition_col = next((c for c in df.columns if c.lower() in (\"position\",\"pos\",\"element_type\",\"player_position\")), None)\nif position_col:\n    df[position_col] = df[position_col].astype(str).str.upper().replace({\"GKP\":\"GK\"})\n    print(\"Position values:\", df[position_col].unique()[:20])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Fill team_x as the PLAYER'S TEAM using opponent-of-opponent within (season, fixture) ===\n# Assumes a DataFrame named `df` is already in memory.\n\nimport pandas as pd\nimport numpy as np\n\n# --- Auto-detect columns ---\nseason_col = next((c for c in df.columns if \"season\" in c.lower()), None)\nfix_col    = next((c for c in df.columns if c.lower() in [\"gw\",\"gameweek\",\"round\",\"event\",\"fixture\"]), None)\n\n# prefer an opponent NAME column (string/object)\nopp_name_col = None\nfor c in df.columns:\n    cl = c.lower()\n    if (\"opp\" in cl or \"opponent\" in cl) and (df[c].dtype == object):\n        opp_name_col = c\n        break\n\nteam_x_col = \"team_x\" if \"team_x\" in df.columns else next((c for c in df.columns if c.lower() == \"team\"), None)\nif team_x_col is None:\n    team_x_col = \"team_x\"\n    df[team_x_col] = np.nan\n\n# --- Opponent-of-opponent mapping per (season, fixture) ---\ndef _norm(x):\n    if pd.isna(x): return x\n    return str(x).strip()\n\nfilled_recip = 0\nif all([season_col, fix_col, opp_name_col, team_x_col]):\n    sub = df[[season_col, fix_col, opp_name_col]].dropna().copy()\n    sub[\"_opp_norm\"] = sub[opp_name_col].map(_norm)\n\n    pair_map = {}  # {(season, fixture): {opp_name -> other_opp_name}}\n    for (s, f), grp in sub.groupby([season_col, fix_col], dropna=False):\n        uniq = list(grp[\"_opp_norm\"].dropna().unique())\n        if len(uniq) == 2:\n            a, b = uniq[0], uniq[1]\n            pair_map[(s, f)] = {a: b, b: a}\n        elif len(uniq) > 2:\n            # fallback: take the two most frequent\n            counts = grp[\"_opp_norm\"].value_counts()\n            top = list(counts.index[:2])\n            if len(top) == 2:\n                pair_map[(s, f)] = {top[0]: top[1], top[1]: top[0]}\n\n    before_nulls = df[team_x_col].isna().sum()\n\n    def infer_team_x(row):\n        if pd.notna(row.get(team_x_col)):\n            return row[team_x_col]\n        key = (row.get(season_col), row.get(fix_col))\n        opp = _norm(row.get(opp_name_col))\n        return pair_map.get(key, {}).get(opp, np.nan)\n\n    df[team_x_col] = df.apply(infer_team_x, axis=1)\n    filled_recip = int(before_nulls - df[team_x_col].isna().sum())\n# ============================================================================================\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Changing Column Names\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Ensure was_home is boolean-like (handles 1/0, True/False, \"True\"/\"False\")\nif df[\"was_home\"].dtype != bool:\n    df[\"was_home\"] = df[\"was_home\"].map({\n        True: True, False: False,\n        1: True, 0: False,\n        \"True\": True, \"False\": False\n    }).fillna(df[\"was_home\"])\n    if df[\"was_home\"].dtype != bool:\n        df[\"was_home\"] = df[\"was_home\"].astype(int).astype(bool)\n\n# Build the two new columns as integers\ndf[\"player_team_score\"] = np.where(df[\"was_home\"], df[\"team_h_score\"], df[\"team_a_score\"]).astype(int)\ndf[\"opp_team_score\"] = np.where(df[\"was_home\"], df[\"team_a_score\"], df[\"team_h_score\"]).astype(int)\n\n# ✅ Convert was_home to integers (1 = home, 0 = away)\ndf[\"was_home\"] = df[\"was_home\"].astype(int)\ndf = df.drop(columns=['team_h_score', 'team_a_score'], errors='ignore')\n\n\n\ndf.info()\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop(columns=['transfers_in', 'transfers_out'], errors='ignore')\n\ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"position_encoded = pd.get_dummies(df['position'], prefix='pos')\n\n# Concatenate back to df and drop the original column\ndf = pd.concat([df.drop(columns=['position']), position_encoded], axis=1)\n\ndf.info()\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pos_cols = [c for c in df.columns if c.startswith('pos_')]\n\n# Convert them to int\ndf[pos_cols] = df[pos_cols].astype(int)\n\n\ndf.info()\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Calculating Form\n### As the average total points over the past four gameweeks (if available), divided by 10","metadata":{}},{"cell_type":"code","source":"# Sort properly\ndf = df.sort_values(by=[\"name\", \"season_x\", \"GW\"])\n\n# Compute the average total points over the previous 4 GWs (divided by 10)\ndf[\"form\"] = (\n    df.groupby([\"name\", \"season_x\"], group_keys=False)\n      .apply(lambda g: g.assign(\n          form=(g[\"total_points\"]\n                .shift(1)                                   # exclude current GW\n                .rolling(window=4, min_periods=1)           # up to 4 previous GWs\n                .mean() / 10)\n      ))\n      [\"form\"]\n)\n\n# Replace NaN (first GW of each season) with 0\ndf[\"form\"] = df[\"form\"].fillna(0)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.a ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n# One-hot position columns in your df\npos_cols = ['pos_DEF', 'pos_FWD', 'pos_GK', 'pos_MID']\n\n# Melt to long form (one row per player-position)\nmelted = df.melt(\n    id_vars=['season_x', 'total_points'],\n    value_vars=pos_cols,\n    var_name='position',\n    value_name='is_pos'\n)\n\n# Keep only rows where the player is that position\nmelted = melted[melted['is_pos'] == 1].copy()\nmelted['position'] = melted['position'].str.replace('pos_', '', regex=False)\n\n# (Optional) nicer labels\npos_label_map = {'DEF': 'Defender', 'FWD': 'Forward', 'GK': 'Goalkeeper', 'MID': 'Midfielder'}\nmelted['position'] = melted['position'].map(pos_label_map).fillna(melted['position'])\n\n# ---- Step 1: Sum points by season & position (seasonal totals per position)\nseason_pos_sum = (\n    melted.groupby(['season_x', 'position'], as_index=False)['total_points']\n          .sum()\n          .rename(columns={'total_points': 'season_sum_points'})\n)\n\n# ---- Step 2: Average those seasonal sums across seasons (answer to the question)\navg_seasonal_sum = (\n    season_pos_sum.groupby('position', as_index=False)['season_sum_points']\n                  .mean()\n                  .rename(columns={'season_sum_points': 'avg_season_sum_points'})\n                  .sort_values('avg_season_sum_points', ascending=False)\n)\n\n# ============================\n# Diagram 1: Average total points per position for each season\n# ============================\navg_points_per_season = (\n    melted.groupby(['season_x', 'position'], as_index=False)['total_points']\n          .mean()\n          .rename(columns={'total_points': 'avg_points_per_player'})\n)\n\npivot_avg = avg_points_per_season.pivot(index='season_x', columns='position', values='avg_points_per_player').fillna(0)\n\npivot_avg.plot(kind='bar', figsize=(10, 5))\nplt.title('Average Total Points per Position for Each Season')\nplt.xlabel('Season')\nplt.ylabel('Average Points per Player')\nplt.legend(title='Position')\nplt.tight_layout()\nplt.show()\n\n# ============================\n# Diagram 2: Seasonal sums per position (grouped bars by season)\n# ============================\npivot = season_pos_sum.pivot(index='season_x', columns='position', values='season_sum_points').fillna(0)\npivot = pivot.sort_index()\n\nseasons = pivot.index.astype(str).tolist()\npositions = pivot.columns.tolist()\nn_pos = len(positions)\nx = np.arange(len(seasons))\nbar_width = 0.8 / max(n_pos, 1)\n\nplt.figure(figsize=(12, 6))\nfor i, pos in enumerate(positions):\n    y = pivot[pos].values\n    plt.bar(x + i*bar_width - (n_pos-1)*bar_width/2, y, width=bar_width, label=pos)\n\nplt.title('Seasonal Sum of Total Points per Position')\nplt.xlabel('Season')\nplt.ylabel('Sum of Total Points (per season)')\nplt.xticks(x, seasons)\nplt.legend(title='Position')\nplt.tight_layout()\nplt.show()\n\n# ============================\n# Diagram 3: Average seasonal sum across seasons (the answer)\n# ============================\nplt.figure(figsize=(8, 5))\nplt.bar(avg_seasonal_sum['position'], avg_seasonal_sum['avg_season_sum_points'])\nplt.title('Average Seasonal Sum of Total Points by Position')\nplt.xlabel('Position')\nplt.ylabel('Average Seasonal Sum of Total Points')\nplt.tight_layout()\nplt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.b","metadata":{}},{"cell_type":"markdown","source":"### Form Evolution","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Select the desired season\nseason_2023 = df[df['season_x'] == '2022-23'].copy()\n\n# Identify top 5 players by total points that season\ntop5_total = (\n    season_2023.groupby('name')['total_points']\n    .sum()\n    .sort_values(ascending=False)\n    .head(5)\n    .index\n)\n\n# Plot form evolution for those players\nplt.figure(figsize=(12, 6))\n\nfor player in top5_total:\n    player_data = (\n        season_2023.loc[season_2023['name'] == player, ['GW', 'form']]\n        .fillna(0)  # 👈 include GW1 (form = 0 by definition)\n        .sort_values('GW')\n    )\n    if not player_data.empty:\n        plt.plot(player_data['GW'], player_data['form'], marker='o', label=player)\n\nplt.title('Form Evolution of Top 5 Total-Point Players (2022–23)')\nplt.xlabel('Gameweek')\nplt.ylabel('Form (average of previous 4 GWs ÷ 10)')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(title='Player', bbox_to_anchor=(1.02, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The top players in form VS the top players with the highest total points","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Filter season\nseason_2023 = df[df['season_x'] == '2022-23'].copy()\n\n# ── Top 5 players by total points ──────────────────────────────\ntop5_total = (\n    season_2023.groupby('name')['total_points']\n    .sum()\n    .sort_values(ascending=False)\n    .head(5)\n)\n\n# ── Top 5 players by highest form (peak form) ─────────────────\ntop5_form_peak = (\n    season_2023.groupby('name')['form']\n    .max()\n    .sort_values(ascending=False)\n    .head(5)\n)\n\n# ── Plot side-by-side figures ─────────────────────────────────\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# --- Chart 1 : Total Points ---\nsns.barplot(\n    x=top5_total.values, y=top5_total.index,\n    ax=axes[0], palette='Blues_r'\n)\naxes[0].set_title('Top 5 Players by Total Points (2022–23)')\naxes[0].set_xlabel('Total Points')\naxes[0].set_ylabel('Player')\naxes[0].grid(True, axis='x', linestyle='--', alpha=0.4)\n\n# --- Chart 2 : Highest Form ---\nsns.barplot(\n    x=top5_form_peak.values, y=top5_form_peak.index,\n    ax=axes[1], palette='Greens_r'\n)\naxes[1].set_title('Top 5 Players by Highest Form (2022–23)')\naxes[1].set_xlabel('Highest Form (Peak)')\naxes[1].set_ylabel('Player')\naxes[1].grid(True, axis='x', linestyle='--', alpha=0.4)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 3","metadata":{}},{"cell_type":"markdown","source":"## Downloading CSV","metadata":{}},{"cell_type":"code","source":"df.to_csv('updated_dataset.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\ndf = pd.read_csv(\"/kaggle/working/updated_dataset.csv\")\n\n# Sort properly by player and week\ndf = df.sort_values(by=[\"name\", \"season_x\", \"GW\"])\n\n# Create upcoming_total_points = next week’s total_points per player\ndf[\"upcoming_total_points\"] = (\n    df.groupby([\"name\", \"season_x\"])[\"total_points\"].shift(-1)\n)\n\n# Drop rows without upcoming_total_points (last GW per player)\ndf = df.dropna(subset=[\"upcoming_total_points\"]).reset_index(drop=True)\n\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('updated_dataset_w_up.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Features","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\n\nmatch_features = [\n    \"minutes\", \"goals_scored\", \"assists\", \"clean_sheets\",\n    \"goals_conceded\", \"saves\", \"bonus\", \"bps\",\n    \"ict_index\", \"influence\", \"creativity\", \"threat\",\n    \"yellow_cards\", \"red_cards\", \"own_goals\",\n    \"penalties_missed\", \"penalties_saved\", \"selected\", \"transfers_balance\",\n    \"was_home\", \"player_team_score\", \"opp_team_score\" \n]\nplayer_features = [\"form\",\"value\",\"pos_DEF\",\"pos_MID\",\"pos_FWD\",\"pos_GK\"]\nfeatures = [c for c in (match_features+player_features) if c in df.columns]\nif \"pos_GK\" in features: features.remove(\"pos_GK\")  # avoid dummy trap\n\nX = df[features].astype(float).values\ny = df[\"upcoming_total_points\"].astype(float).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_test_s  = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add right after Cell 16 (end of the cell)\nposition_feature_map = {\n    \"FWD\": [\n        \"minutes\", \"goals_scored\", \"assists\", \"bonus\", \"bps\",\n        \"influence\", \"creativity\", \"threat\",\n        \"yellow_cards\", \"red_cards\",\"own_goals\",\n        \"form\", \"value\", \"was_home\",\n        \"player_team_score\", \"opp_team_score\", \"penalties_missed\"\n    ],\n    \n    \"MID\": [\n        \"minutes\", \"goals_scored\", \"assists\", \"clean_sheets\", \"bonus\", \"bps\",\n        \"influence\", \"creativity\", \"threat\",\n        \"yellow_cards\", \"red_cards\", \"own_goals\",\n        \"form\", \"value\", \"was_home\",\n        \"player_team_score\", \"opp_team_score\" , \"penalties_missed\"\n    ],\n    \n    \"DEF\": [\n        \"minutes\", \"clean_sheets\", \"goals_conceded\", \"bonus\", \"bps\",\n        \"influence\", \"threat\", \"yellow_cards\", \"red_cards\", \"own_goals\",\n        \"form\", \"value\", \"was_home\",\n        \"player_team_score\", \"opp_team_score\" , \"penalties_missed\"\n    ],\n    \n    \"GK\": [\n        \"minutes\", \"clean_sheets\", \"goals_conceded\", \"saves\", \"bonus\", \"bps\", \"own_goals\"\n        \"yellow_cards\", \"red_cards\",\n        \"form\", \"value\", \"was_home\",\n        \"player_team_score\", \"opp_team_score\" , \"penalties_missed\" , \"penalties_saved\"\n    ]\n}\n\n\n# Helper function to detect player position for a given test index\ndef get_position_from_row(row):\n    if row[df.columns.get_loc(\"pos_FWD\")] == 1: return \"FWD\"\n    if row[df.columns.get_loc(\"pos_MID\")] == 1: return \"MID\"\n    if row[df.columns.get_loc(\"pos_DEF\")] == 1: return \"DEF\"\n    return \"GK\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Train your linear regression model (if not already done)\nlr = LinearRegression()\nlr.fit(X_train_s, y_train)\n\n# Optionally evaluate before SHAP\ny_pred_lr = lr.predict(X_test_s)\nprint(\"Linear Regression trained successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SHAP for linear regression","metadata":{}},{"cell_type":"code","source":"import shap\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fix SHAP compatibility\nif not hasattr(np, \"bool\"): np.bool = bool\nif not hasattr(np, \"str\"):  np.str  = str\n\n# We'll rebuild a mapping from X_test rows back to df:\n# This only works if X_train/X_test were split from df[features] in the same order.\ndf_features_only = df[features].values\n\n# Find matching df row for each X_test row\ntest_df_indices = []\nfor x in X_test:\n    # locate first matching row\n    match_idx = np.where((df_features_only == x).all(axis=1))[0]\n    test_df_indices.append(match_idx[0] if len(match_idx) > 0 else None)\n\n# SHAP per position\nfor pos in position_feature_map.keys():\n\n    print(f\"\\n===================== 📌 SHAP Global Summary for {pos} =====================\")\n\n    # Filter test rows for this position\n    pos_test_indices = []\n    for i, df_idx in enumerate(test_df_indices):\n        if df_idx is not None:\n            orig_row = df.iloc[df_idx]\n            if get_position_from_row(orig_row) == pos:\n                pos_test_indices.append(i)\n\n    if len(pos_test_indices) == 0:\n        print(f\"No test samples for position {pos}, skipping.\")\n        continue\n\n    X_test_pos = X_test_s[pos_test_indices]\n\n    # Background = train (same for all positions, or we can filter later)\n    bg_idx = np.random.RandomState(42).choice(len(X_train_s), size=min(1000, len(X_train_s)), replace=False)\n    background_pos = X_train_s[bg_idx]\n\n    explainer_pos = shap.LinearExplainer(lr, background_pos)\n    shap_vals_pos = explainer_pos(X_test_pos)\n\n    valid_features = [f for f in position_feature_map[pos] if f in features]\n    feature_indices = [features.index(f) for f in valid_features]\n\n    filtered_shap = shap_vals_pos.values[:, feature_indices]\n    filtered_Xtest = X_test_pos[:, feature_indices]\n\n    shap.summary_plot(filtered_shap, filtered_Xtest, feature_names=valid_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SHAP Force Plots (3 samples per position)\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# 🔍 SHAP Force Plots (3 samples per position)\n# ============================\nimport random\nimport shap\n\nfor pos in position_feature_map.keys():\n\n    print(f\"\\n===================== ⚡ SHAP Force Plots for {pos} =====================\")\n\n    # Filter test samples for this position (same logic as above)\n    pos_test_indices = []\n    for i, df_idx in enumerate(test_df_indices):\n        if df_idx is not None:\n            orig_row = df.iloc[df_idx]\n            if get_position_from_row(orig_row) == pos:\n                pos_test_indices.append(i)\n\n    if len(pos_test_indices) == 0:\n        print(f\"No test samples for {pos}, skipping.\")\n        continue\n\n    # Limit to 3 random samples (or all if less than 3)\n    chosen_indices = random.sample(pos_test_indices, min(3, len(pos_test_indices)))\n\n    for idx in chosen_indices:\n        sample = X_test_s[idx:idx+1]\n        # Use the same explainer you defined above for this position\n        shap_values_single = explainer_pos(sample)\n\n        print(f\"\\n▶️ Sample index {idx} ({pos})\")\n        shap.force_plot(\n            explainer_pos.expected_value,\n            shap_values_single.values[0],\n            sample[0],\n            feature_names=features,\n            matplotlib=True\n        )\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LIME for LR","metadata":{}},{"cell_type":"code","source":"# ============================\n# 🔍 LIME Explanations (3 samples per position)\n# ============================\nfrom lime import lime_tabular\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a LIME explainer (works for any regression model)\nlime_explainer = lime_tabular.LimeTabularExplainer(\n    training_data=X_train_s,\n    feature_names=features,\n    mode='regression',\n    verbose=False\n)\n\nfor pos in position_feature_map.keys():\n\n    print(f\"\\n===================== 🟩 LIME Explanations for {pos} =====================\")\n\n    # Get test indices for this position (same logic as SHAP)\n    pos_test_indices = []\n    for i, df_idx in enumerate(test_df_indices):\n        if df_idx is not None:\n            orig_row = df.iloc[df_idx]\n            if get_position_from_row(orig_row) == pos:\n                pos_test_indices.append(i)\n\n    if len(pos_test_indices) == 0:\n        print(f\"No test samples for {pos}, skipping.\")\n        continue\n\n    # Randomly select up to 3 samples\n    chosen_indices = random.sample(pos_test_indices, min(1, len(pos_test_indices)))\n\n    for idx in chosen_indices:\n        sample = X_test_s[idx]\n        print(f\"\\n▶️ Sample index {idx} ({pos})\")\n\n        # Explain prediction\n        exp = lime_explainer.explain_instance(\n            data_row=sample,\n            predict_fn=lr.predict,\n            num_features=10\n        )\n\n        # Display text summary\n        exp.show_in_notebook(show_table=True, show_all=False)\n        # OR use a static plot version if you're running outside Jupyter:\n        fig = exp.as_pyplot_figure()\n        plt.title(f\"LIME Explanation – {pos} (Sample {idx})\")\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FFNN","metadata":{}},{"cell_type":"code","source":"# ============================\n# 🔧 FFNN + SHAP (Per Position) — Drop-in block\n# Requires: df, features, position_feature_map, get_position_from_row,\n#           test_df_indices, X_train_s, X_test_s, y_train, y_test\n# ============================\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport shap\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# --- SHAP compat fixes ---\nif not hasattr(np, \"bool\"): np.bool = bool\nif not hasattr(np, \"str\"):  np.str  = str\n\n# --- Reproducibility ---\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# ============================\n# 1) FFNN: small regression MLP on scaled features\n# ============================\ninput_dim = X_train_s.shape[1]\nffnn = keras.Sequential([\n    layers.Input(shape=(input_dim,)),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dropout(0.15),\n    layers.Dense(32, activation=\"relu\"),\n    layers.Dense(1, activation=\"linear\"),\n])\n\nffnn.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss=\"mse\",\n    metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n)\n\nes  = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\nrlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5)\n\nhist = ffnn.fit(\n    X_train_s, y_train,\n    validation_split=0.2,\n    epochs=200,\n    batch_size=64,\n    callbacks=[es, rlr],\n    verbose=0\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SHAP for FFNN","metadata":{}},{"cell_type":"code","source":"# ============================\n# 🔹 SHAP (Permutation only) + Per-Position Visualization\n# ============================\nimport shap, numpy as np, matplotlib.pyplot as plt\nfrom sklearn.utils import check_random_state\nfrom IPython.display import display\nshap.initjs()\n\nrng = check_random_state(42)\n\n# Small background for speed\nbg_idx = rng.choice(len(X_train_s), size=min(100, len(X_train_s)), replace=False)\nbackground = X_train_s[bg_idx]\n\n# Permutation explainer (fixed)\nmasker = shap.maskers.Independent(background)\nexplainer = shap.Explainer(\n    ffnn, \n    masker=masker, \n    algorithm=\"permutation\",\n    max_evals=2 * X_train_s.shape[1] + 256   # speed/quality tradeoff\n)\nprint(\"✅ Using SHAP PermutationExplainer\")\n\ndef fast_shap_perm(X_input):\n    \"\"\"Permutation SHAP: returns (shap_matrix, base_value_scalar).\"\"\"\n    exp = explainer(X_input)                  # Explanation object\n    vals = np.array(exp.values)\n    if vals.ndim == 1:                        # (n,d) safety\n        vals = vals.reshape(1, -1)\n    base = float(np.mean(np.array(exp.base_values).reshape(-1)))\n    return vals, base\n\n# ============================\n# 🔹 SHAP per Position (summary + 3 force plots)\n# ============================\nfor pos, feats in position_feature_map.items():\n    # Collect test samples for this position\n    pos_idx = [i for i, df_i in enumerate(test_df_indices)\n               if df_i is not None and get_position_from_row(df.iloc[df_i]) == pos]\n    if not pos_idx:\n        print(f\"No samples for {pos}\")\n        continue\n\n    # Sample up to 300 for summary (faster)\n    if len(pos_idx) > 300:\n        pos_idx = rng.choice(pos_idx, 300, replace=False)\n\n    X_pos = X_test_s[pos_idx]\n\n    # Compute SHAP (permutation)\n    shap_vals, base = fast_shap_perm(X_pos)\n\n    # Filter to relevant features for this position\n    valid_feats = [f for f in feats if f in features]\n    idxs = [features.index(f) for f in valid_feats]\n    if not idxs:\n        print(f\"No overlapping features for {pos}, skipping.\")\n        continue\n\n    X_f    = X_pos[:, idxs]\n    shap_f = shap_vals[:, idxs]\n\n    # Summary plot\n    print(f\"\\n📊 SHAP Summary for {pos} (n={len(X_f)})\")\n    shap.summary_plot(shap_f, X_f, feature_names=valid_feats, show=False)\n    plt.title(f\"FFNN — SHAP Summary ({pos})\")\n    plt.tight_layout()\n    plt.show()\n\n    # Up to 3 force plots\n    k = min(3, len(X_f))\n    for i in range(k):\n        try:\n            display(shap.force_plot(base, shap_f[i, :], X_f[i, :],\n                                    feature_names=valid_feats))\n        except Exception as e:\n            print(f\"Force plot {i+1} for {pos} skipped: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LIME for FFNN","metadata":{}},{"cell_type":"code","source":"# ============================\n# 🔍 LIME Explanations for FFNN (3 samples per position)\n# ============================\nfrom lime import lime_tabular\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a LIME explainer on the *scaled* training data\nlime_explainer_ffnn = lime_tabular.LimeTabularExplainer(\n    training_data=X_train_s,\n    feature_names=features,\n    mode='regression',\n    verbose=False\n)\n\nfor pos in position_feature_map.keys():\n    print(f\"\\n===================== 🟩 LIME Explanations for {pos} (FFNN) =====================\")\n\n    # Collect test indices for this position (same logic you used for SHAP)\n    pos_test_indices = []\n    for i, df_idx in enumerate(test_df_indices):\n        if df_idx is not None:\n            orig_row = df.iloc[df_idx]\n            if get_position_from_row(orig_row) == pos:\n                pos_test_indices.append(i)\n\n    if len(pos_test_indices) == 0:\n        print(f\"No test samples for {pos}, skipping.\")\n        continue\n\n    # Randomly select up to 3 samples\n    chosen_indices = random.sample(pos_test_indices, min(1, len(pos_test_indices)))\n\n    for idx in chosen_indices:\n        sample = X_test_s[idx]\n        print(f\"\\n▶️ Sample index {idx} ({pos})\")\n\n        # LIME expects predict_fn -> (n, ) for regression; flatten Keras output\n        exp = lime_explainer_ffnn.explain_instance(\n            data_row=sample,\n            predict_fn=lambda X: ffnn.predict(X, verbose=0).ravel(),\n            num_features=10\n        )\n\n        # Notebook-friendly view + static plot\n        exp.show_in_notebook(show_table=True, show_all=False)\n        fig = exp.as_pyplot_figure()\n        plt.title(f\"LIME Explanation – {pos} (FFNN, Sample {idx})\")\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Linear Regression vs FFNN — Full Evaluation Block\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# 📊 Linear Regression vs FFNN — Full Evaluation Block\n# ============================\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# ---------- 1) GLOBAL METRICS ----------\ndef reg_metrics(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    return pd.Series({\n        \"MAE\": mean_absolute_error(y_true, y_pred),\n        \"MSE\": mse,\n        \"RMSE\": np.sqrt(mse),\n        \"R²\": r2_score(y_true, y_pred)\n    })\n\n# fresh predictions\ny_pred_lr   = lr.predict(X_test_s).ravel()\ny_pred_ffnn = ffnn.predict(X_test_s).ravel()\n\n# compute global metrics\nglobal_lr  = reg_metrics(y_test, y_pred_lr)\nglobal_ff  = reg_metrics(y_test, y_pred_ffnn)\nglobal_df = pd.DataFrame([global_lr, global_ff], index=[\"Linear Regression\", \"FFNN\"]).round(4)\n\nprint(\"========= 📈 Overall Model Performance =========\")\nprint(global_df)\nprint()\n\n# ---------- 2) PER-POSITION METRICS ----------\npos_masks = {\"FWD\": [], \"MID\": [], \"DEF\": [], \"GK\": []}\nfor i, df_idx in enumerate(test_df_indices):\n    if df_idx is None: continue\n    pos = get_position_from_row(df.iloc[df_idx])\n    if pos in pos_masks: pos_masks[pos].append(i)\n\nrows = []\nfor pos, idxs in pos_masks.items():\n    if not idxs: continue\n    lr_m = reg_metrics(y_test[idxs],  lr.predict(X_test_s[idxs]).ravel())\n    ff_m = reg_metrics(y_test[idxs],  ffnn.predict(X_test_s[idxs]).ravel())\n    rows += [\n        {\"Position\": pos, \"Model\": \"Linear Regression\", **lr_m.to_dict()},\n        {\"Position\": pos, \"Model\": \"FFNN\", **ff_m.to_dict()}\n    ]\n\nper_pos = pd.DataFrame(rows).round(4)\nprint(\"========= ⚽ Per-Position Metrics =========\")\nprint(per_pos.pivot(index=\"Position\", columns=\"Model\", values=\"MAE\"))\nprint()\n\n# ---------- 3) BAR CHART: LR vs FFNN ----------\nmetrics = [\"MAE\", \"RMSE\", \"R²\"]\nlr_vals  = [global_lr[m] for m in metrics]\nff_vals  = [global_ff[m] for m in metrics]\n\nx = np.arange(len(metrics))\nw = 0.35\nfig, ax = plt.subplots(figsize=(7,5))\nax.bar(x - w/2, lr_vals, w, label=\"Linear Regression\")\nax.bar(x + w/2, ff_vals, w, label=\"FFNN\")\nax.set_xticks(x); ax.set_xticklabels(metrics)\nax.set_ylabel(\"Score\"); ax.set_title(\"📊 Linear Regression vs FFNN Performance\")\nax.legend(); plt.tight_layout(); plt.show()\n\n# ---------- 4) RESIDUAL PLOTS ----------\nfor name, yhat in [(\"Linear Regression\", y_pred_lr), (\"FFNN\", y_pred_ffnn)]:\n    res = y_test - yhat\n    plt.figure(figsize=(6,4))\n    plt.scatter(yhat, res, s=8, alpha=0.5)\n    plt.axhline(0, ls=\"--\", color=\"red\")\n    plt.title(f\"Residuals vs Predictions — {name}\")\n    plt.xlabel(\"Predicted Points\"); plt.ylabel(\"Residuals\")\n    plt.tight_layout(); plt.show()\n\n# ---------- 5) BOOTSTRAP ΔMAE SIGNIFICANCE ----------\nrng = np.random.RandomState(42)\ndeltas = []\nfor _ in range(2000):\n    idx = rng.randint(0, len(y_test), len(y_test))\n    mae_lr_b  = np.mean(np.abs(y_test[idx] - y_pred_lr[idx]))\n    mae_ff_b  = np.mean(np.abs(y_test[idx] - y_pred_ffnn[idx]))\n    deltas.append(mae_lr_b - mae_ff_b)\n\nlo, hi = np.percentile(deltas, [2.5, 97.5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference Function","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Fallback order (only used if you don't pass feature_order and no global `features` exists)\n_FALLBACK_FEATURES = [\n    'minutes','goals_scored','assists','saves','goals_conceded','clean_sheets',\n    'penalties_missed','penalties_saved','yellow_cards','red_cards','own_goals',\n    'bps','bonus','ict_index','influence','creativity','threat',\n    'player_team_score','opp_team_score','selected','transfers_balance','value','form','was_home',\n    'pos_DEF','pos_MID','pos_FWD','pos_GK'\n    # NOTE: 'total_points' is a label column and is intentionally excluded from inputs.\n]\n\ndef _resolve_feature_order(feature_order=None):\n    if feature_order is not None:\n        return feature_order\n    try:\n        return features  # the exact list you used when building X\n    except NameError:\n        return _FALLBACK_FEATURES\n\ndef _ensure_feature_order(df: pd.DataFrame, order: list) -> pd.DataFrame:\n    for c in order:\n        if c not in df.columns:\n            df[c] = 0\n    # drop extra columns (like 'total_points' if present)\n    return df[order]\n\ndef _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n    for c in df.columns:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df.fillna(0)\n\ndef _describe_points(p: float) -> str:\n    if p >= 10: tier = \"massive haul potential\"\n    elif p >= 8: tier = \"strong return expected\"\n    elif p >= 5: tier = \"solid chance of points\"\n    elif p >= 3: tier = \"modest outlook\"\n    else: tier = \"low expectation this GW\"\n    return f\"Predicted {p:.2f} points — {tier}.\"\n\ndef _lookup_model(model_choice: str | None):\n    \"\"\"Resolve a model by common names in globals().\"\"\"\n    if not model_choice:\n        return None\n    choice = model_choice.lower()\n    name_map = {\n        # linear regression aliases\n        \"linearregression\": \"lr\", \"linear\": \"lr\", \"linreg\": \"lr\", \"lr\": \"lr\",\n        # ffnn aliases\n        \"ffnn\": \"ffnn\", \"nn\": \"ffnn\", \"keras\": \"ffnn\", \"mlp\": \"ffnn\"\n    }\n    var_name = name_map.get(choice)\n    return globals().get(var_name) if var_name else None\n\ndef predict_upcoming_points(\n    raw_input: dict,\n    model=None,                 # pass your trained model object here (e.g., lr or ffnn)\n    model_choice: str | None = None,  # or pass a string like 'lr' or 'ffnn'\n    scaler=None,                # pass the fitted StandardScaler / ColumnTransformer / Pipeline if used\n    feature_order: list | None = None,\n    clamp_zero: bool = True\n):\n    \"\"\"\n    Apply the SAME preprocessing as training, run the model, and return a friendly summary.\n\n    raw_input: dict of raw fields (see the examples)\n    model: trained model object (sklearn or Keras). Preferred.\n    model_choice: alternative lookup by name ('lr' or 'ffnn') if you don't pass `model`\n    scaler: the exact transformer/pipeline used in training (if any). REQUIRED if you trained on scaled features.\n    feature_order: pass your exact training column order (recommended); else uses global `features` or a fallback.\n    \"\"\"\n    # Resolve model\n    mdl = model or _lookup_model(model_choice)\n    if mdl is None:\n        raise ValueError(\"No model provided. Pass `model=lr` or `model=ffnn`, \"\n                         \"or use `model_choice='lr'|'ffnn'` after defining those variables.\")\n\n    # Build a 1-row DataFrame from the dict\n    x_df = pd.DataFrame([raw_input]).copy()\n    # Ensure label column isn't fed as input\n    if 'total_points' in x_df.columns:\n        x_df = x_df.drop(columns=['total_points'])\n\n    # Order columns exactly like training\n    order = _resolve_feature_order(feature_order)\n    x_df = _coerce_numeric(x_df)\n    x_df = _ensure_feature_order(x_df, order)\n\n    # Apply the same scaling/pipeline\n    x_proc = x_df\n    if scaler is not None:\n        try:\n            x_proc = scaler.transform(x_df)\n        except Exception:\n            x_proc = scaler.transform(np.asarray(x_df, dtype=float))\n\n    # Predict (sklearn or Keras)\n    if hasattr(mdl, \"predict\"):\n        y_pred = mdl.predict(x_proc)\n        pred = float(np.ravel(y_pred)[0])\n    else:\n        x_np = np.asarray(x_proc, dtype=np.float32)\n        pred = float(mdl(x_np, training=False).numpy().reshape(-1)[0])\n\n    if clamp_zero:\n        pred = max(pred, 0.0)\n\n    return {\"numeric\": pred, \"text\": _describe_points(round(pred, 2))}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TEST","metadata":{}},{"cell_type":"code","source":"example_input_1 = {\n  \"minutes\": 0,\n  \"goals_scored\": 0,\n  \"assists\": 0,\n  \"clean_sheets\": 0,\n  \"goals_conceded\": 0,\n  \"saves\": 0,\n  \"bonus\": 0,\n  \"bps\": 0,\n  \"ict_index\": 0.0,\n  \"influence\": 0.0,\n  \"creativity\": 0.0,\n  \"threat\": 0.0,\n  \"yellow_cards\": 0,\n  \"red_cards\": 0,\n  \"own_goals\": 0,\n  \"penalties_missed\": 0,\n  \"penalties_saved\": 0,\n  \"selected\": 7288,\n  \"transfers_balance\": 6737,\n  \"was_home\": 0,\n  \"player_team_score\": 1,\n  \"opp_team_score\": 2,\n  \"form\": 0.0,\n  \"value\": 45,\n  \"pos_DEF\": 0,\n  \"pos_MID\": 0,\n  \"pos_FWD\": 0\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_input_2 = {\n  \"minutes\": 90,\n  \"goals_scored\": 0,\n  \"assists\": 0,\n  \"clean_sheets\": 0,\n  \"goals_conceded\": 1,\n  \"saves\": 0,\n  \"bonus\": 0,\n  \"bps\": 3,\n  \"ict_index\": 2.3,\n  \"influence\": 6.0,\n  \"creativity\": 0.0,\n  \"threat\": 0.0,\n  \"yellow_cards\": 0,\n  \"red_cards\": 0,\n  \"own_goals\": 0,\n  \"penalties_missed\": 0,\n  \"penalties_saved\": 0,\n  \"selected\": 37458,\n  \"transfers_balance\": 29157,\n  \"was_home\": 1,\n  \"player_team_score\": 0,\n  \"opp_team_score\": 1,\n  \"form\": 0.1,\n  \"value\": 45,\n  \"pos_DEF\": 1,\n  \"pos_MID\": 0,\n  \"pos_FWD\": 0\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_input_3 = {\n  \"minutes\": 24,\n  \"goals_scored\": 0,\n  \"assists\": 0,\n  \"clean_sheets\": 0,\n  \"goals_conceded\": 1,\n  \"saves\": 0,\n  \"bonus\": 0,\n  \"bps\": 0,\n  \"ict_index\": 1.3,\n  \"influence\": 1.3,\n  \"creativity\": 0.0,\n  \"threat\": 0.0,\n  \"yellow_cards\": 0,\n  \"red_cards\": 0,\n  \"own_goals\": 0,\n  \"penalties_missed\": 0,\n  \"penalties_saved\": 0,\n  \"selected\": 48351,\n  \"transfers_balance\": 35343,\n  \"was_home\": 0,\n  \"player_team_score\": 1,\n  \"opp_team_score\": 2,\n  \"form\": 0.1,\n  \"value\": 50,\n  \"pos_DEF\": 0,\n  \"pos_MID\": 1,\n  \"pos_FWD\": 0\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_input_4 = {\n  \"minutes\": 90,\n  \"goals_scored\": 1,\n  \"assists\": 1,\n  \"clean_sheets\": 0,\n  \"goals_conceded\": 3,\n  \"saves\": 0,\n  \"bonus\": 0,\n  \"bps\": 17,\n  \"ict_index\": 15.6,\n  \"influence\": 27.6,\n  \"creativity\": 0.1,\n  \"threat\": 44.0,\n  \"yellow_cards\": 0,\n  \"red_cards\": 0,\n  \"own_goals\": 0,\n  \"penalties_missed\": 0,\n  \"penalties_saved\": 0,\n  \"selected\": 69457,\n  \"transfers_balance\": 6749,\n  \"was_home\": 0,\n  \"player_team_score\": 1,\n  \"opp_team_score\": 3,\n  \"form\": 2.0,\n  \"value\": 55,\n  \"pos_DEF\": 0,\n  \"pos_MID\": 0,\n  \"pos_FWD\": 1\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_input_5 = {\n  \"minutes\": 90,\n  \"goals_scored\": 4,\n  \"assists\": 1,\n  \"clean_sheets\": 1,\n  \"goals_conceded\": 0,\n  \"saves\": 0,\n  \"bonus\": 3,\n  \"bps\": 57,\n  \"ict_index\": 23.5,\n  \"influence\": 73.0,\n  \"creativity\": 27.0,\n  \"threat\": 102.0,\n  \"yellow_cards\": 0,\n  \"red_cards\": 0,\n  \"own_goals\": 0,\n  \"penalties_missed\": 0,\n  \"penalties_saved\": 0,\n  \"selected\": 3662419,\n  \"transfers_balance\": 62759,\n  \"was_home\": 1,\n  \"player_team_score\": 5,\n  \"opp_team_score\": 0,\n  \"form\": 0.825,\n  \"value\": 106,\n  \"pos_DEF\": 0,\n  \"pos_MID\": 1,\n  \"pos_FWD\": 0\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_input = {\n    # Match-related stats (from match_features list)\n    'minutes': 21, # if >=60 = 2 pts. <60 = 1 pts. 0 = 0 pts\n    'goals_scored': 1, # 4 for fwd / 5 for mid / 6 for def and gw\n    'assists': 0, # 3 pts \n    'saves': 0, # every 3 saves = 1 pt\n    'goals_conceded': 0, # every 2 = -1 pt for def and gw\n    'clean_sheets': 0, # 4 for def and gw / 1 for mid\n    'penalties_missed': 0, # -2 pts\n    'penalties_saved': 0, # 5 pts for gw\n    'yellow_cards': 0, # -1 pts\n    'red_cards': 0, # -3 pts\n    'own_goals': 0, # -2 pts\n    'bps': 26,\n    'bonus': 1, # these are bonus points added based on performance\n    'total_points': 7,\n    'ict_index': 6.1,\n    'influence': 38.2,\n    'creativity': 1.9,\n    'threat': 21.0,\n    'player_team_score': 1,\n    'opp_team_score': 2,\n    'selected': 15000,\n    'transfers_balance': 2000,\n    'value': 50,\n    'form': 2.5,\n    'was_home': 0,  # 1 = home, 0 = away\n\n    # Example position one-hot encoding\n    'pos_DEF': 0,\n    'pos_MID': 1,\n    'pos_FWD': 0,\n    'pos_GK': 0,\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TEST Fn","metadata":{}},{"cell_type":"code","source":"print(\"\\nLinear Regression Prediction:\")\npredict_upcoming_points(example_input_4, model=lr, scaler=scaler, feature_order=features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nFFNN Prediction:\")\npredict_upcoming_points(example_input_4, model=ffnn, scaler=scaler, feature_order=features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference Function (CSV)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef notebook_style_infer(\n    data,                         # str path to CSV or a preloaded DataFrame\n    model=None,                   # pass your trained model object: lr or ffnn (required)\n    scaler=None,                  # your fitted StandardScaler / ColumnTransformer / Pipeline (required for scaled models)\n    feature_order=None,           # EXACT list you trained with (recommended); else we’ll derive and align\n    clamp_zero: bool = True,      # clip negatives to 0 for FPL display\n    return_columns: list | None = None  # extra columns to return alongside predictions\n):\n    \"\"\"\n    One-shot pipeline:\n      Load/clean -> engineer (team scores, one-hot positions, form) -> align features -> scale with your fitted scaler\n      -> predict with your trained model -> return frame with predictions (and actual upcoming_total_points if present)\n\n    IMPORTANT:\n    - This does NOT retrain anything.\n    - Expects you to pass the trained `model` and the fitted `scaler` from your notebook.\n    - Uses your original `feature_order` (the `features` you trained with). If not supplied, we attempt to infer it\n      from the cleaned dataframe based on your training rules (pos_GK removed).\n    \"\"\"\n\n    # -------------------------\n    # 0) Ingest\n    # -------------------------\n    if isinstance(data, str):\n        df = pd.read_csv(data)\n    elif isinstance(data, pd.DataFrame):\n        df = data.copy()\n    else:\n        raise TypeError(\"`data` must be a CSV path or a pandas DataFrame.\")\n\n    # -------------------------\n    # 1) Position normalization (GKP -> GK)\n    # -------------------------\n    position_col = next((c for c in df.columns if c.lower() in (\"position\",\"pos\",\"element_type\",\"player_position\")), None)\n    if position_col:\n        df[position_col] = df[position_col].astype(str).str.upper().replace({\"GKP\":\"GK\"})\n\n    # -------------------------\n    # 2) Fill player team (team_x) via opponent-of-opponent within (season, fixture)\n    #    (works best when there are exactly 2 opponent names per fixture)\n    # -------------------------\n    season_col = next((c for c in df.columns if \"season\" in c.lower()), None)\n    fix_col    = next((c for c in df.columns if c.lower() in [\"gw\",\"gameweek\",\"round\",\"event\",\"fixture\"]), None)\n\n    opp_name_col = None\n    for c in df.columns:\n        cl = c.lower()\n        if (\"opp\" in cl or \"opponent\" in cl) and (df[c].dtype == object):\n            opp_name_col = c\n            break\n\n    team_x_col = \"team_x\" if \"team_x\" in df.columns else next((c for c in df.columns if c.lower() == \"team\"), None)\n    if team_x_col is None:\n        team_x_col = \"team_x\"\n        df[team_x_col] = np.nan\n\n    def _norm(x):\n        if pd.isna(x): return x\n        return str(x).strip()\n\n    if all([season_col, fix_col, opp_name_col, team_x_col]):\n        sub = df[[season_col, fix_col, opp_name_col]].dropna().copy()\n        sub[\"_opp_norm\"] = sub[opp_name_col].map(_norm)\n\n        pair_map = {}\n        for (s, f), grp in sub.groupby([season_col, fix_col], dropna=False):\n            uniq = list(grp[\"_opp_norm\"].dropna().unique())\n            if len(uniq) == 2:\n                a, b = uniq[0], uniq[1]\n                pair_map[(s, f)] = {a: b, b: a}\n            elif len(uniq) > 2:\n                counts = grp[\"_opp_norm\"].value_counts()\n                top = list(counts.index[:2])\n                if len(top) == 2:\n                    pair_map[(s, f)] = {top[0]: top[1], top[1]: top[0]}\n\n        def infer_team_x(row):\n            if pd.notna(row.get(team_x_col)):\n                return row[team_x_col]\n            key = (row.get(season_col), row.get(fix_col))\n            opp = _norm(row.get(opp_name_col))\n            return pair_map.get(key, {}).get(opp, np.nan)\n\n        df[team_x_col] = df.apply(infer_team_x, axis=1)\n\n    # -------------------------\n    # 3) was_home normalization + team scores columns\n    # -------------------------\n    if \"was_home\" in df.columns:\n        if df[\"was_home\"].dtype != bool:\n            df[\"was_home\"] = df[\"was_home\"].map({\n                True: True, False: False, 1: True, 0: False, \"True\": True, \"False\": False\n            }).fillna(df[\"was_home\"])\n            if df[\"was_home\"].dtype != bool:\n                df[\"was_home\"] = df[\"was_home\"].astype(int).astype(bool)\n\n        # Create player_team_score / opp_team_score when team_h_score/team_a_score exist\n        if {\"team_h_score\",\"team_a_score\"}.issubset(df.columns):\n            df[\"player_team_score\"] = np.where(df[\"was_home\"], df[\"team_h_score\"], df[\"team_a_score\"]).astype(int)\n            df[\"opp_team_score\"]    = np.where(df[\"was_home\"], df[\"team_a_score\"], df[\"team_h_score\"]).astype(int)\n            # Convert was_home to 1/0 as per training\n            df[\"was_home\"] = df[\"was_home\"].astype(int)\n            df.drop(columns=[\"team_h_score\",\"team_a_score\"], inplace=True, errors=\"ignore\")\n        else:\n            # If scores already exist, just coerce was_home to int\n            df[\"was_home\"] = df[\"was_home\"].astype(int)\n    else:\n        # If missing, create neutral defaults\n        df[\"was_home\"] = 0\n        if \"player_team_score\" not in df.columns: df[\"player_team_score\"] = 0\n        if \"opp_team_score\"    not in df.columns: df[\"opp_team_score\"]    = 0\n\n    # -------------------------\n    # 4) Drop columns you’ve removed during training\n    # -------------------------\n    df.drop(columns=[\"transfers_in\",\"transfers_out\"], inplace=True, errors=\"ignore\")\n\n    # -------------------------\n    # 5) One-hot encode positions => pos_DEF/MID/FWD/GK (int)\n    # -------------------------\n    if position_col and position_col in df.columns:\n        position_encoded = pd.get_dummies(df[position_col], prefix=\"pos\")\n        df = pd.concat([df.drop(columns=[position_col]), position_encoded], axis=1)\n\n    pos_cols = [c for c in df.columns if c.startswith(\"pos_\")]\n    if pos_cols:\n        df[pos_cols] = df[pos_cols].astype(int)\n\n    # -------------------------\n    # 6) Sort & compute FORM (rolling mean of previous 4 GWs / 10)\n    # -------------------------\n    # Infer common id/time columns if present\n    name_col   = next((c for c in df.columns if c.lower() == \"name\"), None)\n    season_col = season_col or next((c for c in df.columns if \"season\" in c.lower()), None)\n    gw_col     = next((c for c in df.columns if c in (\"GW\",\"gw\",\"round\",\"event\")), None)\n\n    if name_col and season_col and gw_col and \"total_points\" in df.columns:\n        df = df.sort_values(by=[name_col, season_col, gw_col])\n        df[\"form\"] = (\n            df.groupby([name_col, season_col], group_keys=False)\n              .apply(lambda g: g.assign(\n                  form=(g[\"total_points\"].shift(1).rolling(window=4, min_periods=1).mean() / 10)\n              ))[\"form\"]\n        ).fillna(0)\n    else:\n        # Fallback if structure is missing: keep existing 'form' or set 0\n        if \"form\" not in df.columns:\n            df[\"form\"] = 0.0\n\n    # -------------------------\n    # 7) Create upcoming_total_points label (for optional evaluation)\n    # -------------------------\n    if name_col and season_col and gw_col and \"total_points\" in df.columns:\n        df[\"upcoming_total_points\"] = (\n            df.groupby([name_col, season_col])[\"total_points\"].shift(-1)\n        )\n        # Keep label if you want to compare later; do not drop\n\n    # -------------------------\n    # 8) Build the exact training feature list (your rule: remove pos_GK to avoid dummy trap)\n    # -------------------------\n    if feature_order is not None:\n        features_final = list(feature_order)\n    else:\n        match_features = [\n            \"minutes\",\"goals_scored\",\"assists\",\"clean_sheets\",\n            \"goals_conceded\",\"saves\",\"bonus\",\"bps\",\n            \"ict_index\",\"influence\",\"creativity\",\"threat\",\n            \"yellow_cards\",\"red_cards\",\"own_goals\",\n            \"penalties_missed\",\"penalties_saved\",\"selected\",\"transfers_balance\",\n            \"was_home\",\"player_team_score\",\"opp_team_score\"\n        ]\n        player_features = [\"form\",\"value\",\"pos_DEF\",\"pos_MID\",\"pos_FWD\",\"pos_GK\"]\n        features_final = [c for c in (match_features + player_features) if c in df.columns]\n        if \"pos_GK\" in features_final:\n            features_final.remove(\"pos_GK\")\n\n    # -------------------------\n    # 9) Assemble X, align numerics, and drop rows with missing required inputs\n    # -------------------------\n    Xdf = df[features_final].copy()\n    for c in Xdf.columns:\n        Xdf[c] = pd.to_numeric(Xdf[c], errors=\"coerce\")\n    Xdf = Xdf.fillna(0)\n\n    # -------------------------\n    # 10) Apply your fitted scaler / pipeline (no refit here)\n    # -------------------------\n    X_proc = Xdf\n    if scaler is not None:\n        try:\n            X_proc = scaler.transform(Xdf)\n        except Exception:\n            X_proc = scaler.transform(np.asarray(Xdf, dtype=float))\n\n    # -------------------------\n    # 11) Predict with your trained model (sklearn or Keras)\n    # -------------------------\n    if model is None:\n        raise ValueError(\"Please pass your trained model object (e.g., model=lr or model=ffnn).\")\n\n    if hasattr(model, \"predict\"):\n        preds = model.predict(X_proc).reshape(-1)\n    else:\n        preds = model(np.asarray(X_proc, dtype=np.float32), training=False).numpy().reshape(-1)\n\n    if clamp_zero:\n        preds = np.maximum(preds, 0.0)\n\n    # -------------------------\n    # 12) Return a tidy result frame with optional extras\n    # -------------------------\n    out = pd.DataFrame({\n        \"predicted_upcoming_total_points\": preds\n    })\n\n    # Add identifiers if present (handy)\n    for c in [name_col, season_col, gw_col]:\n        if c:\n            out[c] = df[c].values\n\n    # Include actual upcoming_total_points if available (for evaluation)\n    if \"upcoming_total_points\" in df.columns:\n        out[\"actual_upcoming_total_points\"] = df[\"upcoming_total_points\"].values\n\n    # Reorder columns nicely\n    pretty_cols = [c for c in [name_col, season_col, gw_col] if c] + \\\n                  [\"predicted_upcoming_total_points\"] + \\\n                  ([\"actual_upcoming_total_points\"] if \"actual_upcoming_total_points\" in out.columns else [])\n    out = out[pretty_cols + [c for c in out.columns if c not in pretty_cols]]\n\n    return out, features_final\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_df, used_features = notebook_style_infer(\n    data=\"/kaggle/input/fantasy-football/cleaned_merged_seasons.csv\", \n    model=ffnn,      \n    scaler=scaler,   \n    feature_order=features\n)\n\npreds_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_df, used_features = notebook_style_infer(\n    data=\"/kaggle/input/fantasy-football/cleaned_merged_seasons.csv\",\n    model=lr,      \n    scaler=scaler,   \n    feature_order=features\n)\n\npreds_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}